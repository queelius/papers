{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "- Check your system and make sure it satisfies the following requirements:\n",
    "\n",
    "  - This package should work on any Linux distribution. Sorry no MacOS or Windows support :)\n",
    "  - Supported architectures are x86_64 and i686.\n",
    "  - Your system needs to be little-endian. This should be the case for most modern machines.\n",
    "  - Please make sure you have Python >=3.8 (and strictly speaking, CPython, not PyPy or some other implementations).\n",
    "\n",
    "- Install this package: pip install infini-gram\n",
    "\n",
    "- Download the infini-gram index that you would like to query. For sake of performance, it is strongly recommended that you put the index on an SSD. See details in the `Pre-built Indexes` section below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-built Indexes\n",
    "\n",
    "We have made the following indexes publicly available on AWS S3.\n",
    "\n",
    "Smaller indexes are stored in the `<s3://infini-gram-lite>` bucket and can be downloaded for free and without an AWS account. These indexes are `v4_pileval_llama`, `v4_pileval_gpt2`, and `v4_dolmasample_olmo`. To download, run command:\n",
    "\n",
    "```bash\n",
    "aws s3 cp --no-sign-request --recursive {S3_URL} {LOCAL_INDEX_PATH}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n",
    "\n",
    "Prior to submitting any type of queries, you need to instatiate the engine with the index you would like to query. As an example, below we create an engine with the index for Pile-val (the validation set of Pile), which was created using the Llama-2 tokenizer.\n",
    "\n",
    "Let's load our tokenizer. The tokenizer should match that of the index you load\n",
    "for the Infini-gram AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", add_bos_token=False, add_eos_token=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load our Infini-gram model using the `tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from infini_gram.engine import InfiniGramEngine\n",
    " # replace `index_dir` with the local directory where you store the index\n",
    "engine = InfiniGramEngine(index_dir='index/v4_pileval_llama',\n",
    "                          eos_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count an n-gram (count() and count_cnf())\n",
    "\n",
    "This query type counts the number of times the query string appears in the corpus. For example, to find out the number of occurrences of n-gram natural language processing in the Pile-val corpus,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5613, 4086, 9068]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('natural language processing')\n",
    "print(input_ids)\n",
    "# [5613, 4086, 9068]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 76, 'approx': False}\n"
     ]
    }
   ],
   "source": [
    "out = engine.count(input_ids=input_ids)\n",
    "print(out)\n",
    "# {'count': 76, 'approx': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `approx` field indicates whether the count is approximate. For simple queries with a single n-gram term, this is always False (the count is always exact). As you will see later, count for complex queries may be approximate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empty Query\n",
    "\n",
    "If you submit an empty query, the engine returns the total number of tokens in the corpus.\n",
    "The empty query is just `[]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'count': 393769120, 'approx': False}\n"
     ]
    }
   ],
   "source": [
    "print(engine.count(input_ids=[]))\n",
    "#{'count': 393769120, 'approx': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also make more complex queries by connecting multiple n-grams with the `AND` and `OR` operators, in the CNF format, in which case the engine counts the number of times where this logical constraint is satisfied. A CNF query is a triply-nested list. The top-level is a list of disjunctive clauses (which are eventually connected with the AND operator). Each disjuctive clause is a list of n-gram terms (which are eventually connected with the OR operator). And each n-gram term has the same format as input_ids above, i.e., a list of token ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5613, 4086, 9068], [23116, 21082]]]\n",
      "{'count': 499, 'approx': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# natural language processing OR artificial intelligence\n",
    "cnf = [\n",
    "    [tokenizer.encode('natural language processing'), tokenizer.encode('artificial intelligence')]\n",
    "]\n",
    "print(cnf)\n",
    "# [[[5613, 4086, 9068], [23116, 21082]]]\n",
    "\n",
    "print(engine.count_cnf(cnf=cnf))\n",
    "# {'count': 499, 'approx': False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_cnf(json):\n",
    "    cnf = []\n",
    "    for disj in json:\n",
    "        cnf.append([tokenizer.encode(phrase) for phrase in disj])\n",
    "    return cnf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5613, 4086, 9068], [23116, 21082]]]\n"
     ]
    }
   ],
   "source": [
    "print(tokenize_cnf([['natural language processing', 'artificial intelligence']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5613, 4086, 9068], [23116, 21082]], [[6483, 6509]]]\n",
      "{'count': 19, 'approx': False}\n"
     ]
    }
   ],
   "source": [
    "# (natural language processing OR artificial intelligence) AND deep learning\n",
    "cnf = [\n",
    "   [tokenizer.encode('natural language processing'), tokenizer.encode('artificial intelligence')],\n",
    "   [tokenizer.encode('deep learning')]\n",
    "]\n",
    "print(cnf)\n",
    "# [[[5613, 4086, 9068], [23116, 21082]], [[6483, 6509]]]\n",
    "\n",
    "print(engine.count_cnf(cnf=cnf))\n",
    "#{'count': 19, 'approx': False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'count': 19, 'approx': False}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "engine.count_cnf(tokenize_cnf([['natural language processing', 'artificial intelligence'], ['deep learning']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability of an n-gram\n",
    "\n",
    "This query type computes the n-gram LM probability of a token conditioning on a preceding prompt.\n",
    "\n",
    "For example, to compute P(processing | natural language):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5613, 4086, 9068]\n",
      "{'prompt_cnt': 257, 'cont_cnt': 76, 'prob': 0.29571984435797666}\n",
      "{'prompt_cnt': 257, 'cont_cnt': 76, 'prob': 0.29571984435797666}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('natural language processing')\n",
    "print(input_ids)\n",
    "# [5613, 4086, 9068]\n",
    "\n",
    "natural_id = tokenizer.encode('natural')[-1]\n",
    "language_id = tokenizer.encode('language')[-1]\n",
    "processing_id = tokenizer.encode('processing')[-1]\n",
    "\n",
    "\n",
    "print(engine.prob(prompt_ids=input_ids[:-1], cont_id=input_ids[-1]))\n",
    "#{'prompt_cnt': 257, 'cont_cnt': 76, 'prob': 0.29571984435797666}\n",
    "\n",
    "res = engine.prob(prompt_ids=[natural_id, language_id], cont_id=processing_id)\n",
    "print(res)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a helper function for computing joint probabilities of n-grams.\n",
    "`engine.prob()` returns the probability of the last token in the query given the preceding tokens. The query is a list of token ids. So: Prob[d c | a b] = Prob[d | a b c] * Prob[c | a b].\n",
    "In code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conditional_prob(prompt_ids, cont_ids):\n",
    "    \"\"\"\n",
    "    Compute the conditional probability of a continuation sequence given a prompt sequence\n",
    "\n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    natural_id, language_id, processing_id, is_id, fun_id =\n",
    "        tokenizer.encode('natural language processing is fun')\n",
    "    compute_conditional_prob([natural_id, language_id], [processing_id, is_id, fun_id])\n",
    "\n",
    "\n",
    "    :param prompt_ids: list of token ids representing the prompt sequence\n",
    "    :param cont_ids: list of token ids representing the continuation sequence\n",
    "    :return: the conditional probability of the continuation sequence given the prompt sequence\n",
    "    \"\"\"\n",
    "    prob = 1\n",
    "    orig = prompt_ids.copy()\n",
    "    for tok in cont_ids:\n",
    "        p = engine.prob(prompt_ids=prompt_ids, cont_id=tok)['prob']\n",
    "        prob *= p\n",
    "        prompt_ids.append(tok)\n",
    "    return {'prompt_cnt': orig, 'cont_cnt': cont_ids, 'prob': prob}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5613, 4086, 9068]\n",
      "{'prompt_cnt': [], 'cont_cnt': [5613, 4086, 9068], 'prob': 1.930065008652786e-07}\n"
     ]
    }
   ],
   "source": [
    "natural_id, language_id, processing_id = tokenizer.encode('natural language processing')\n",
    "print([natural_id, language_id, processing_id])\n",
    "res = compute_conditional_prob([], [natural_id, language_id, processing_id])\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next-token distribution (ntd())\n",
    "\n",
    "This query type computes the n-gram LM next-token distribution conditioning on a preceding prompt.\n",
    "\n",
    "For example, this will return the token distribution following natural language:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5613, 4086]\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('natural language')\n",
    "print(input_ids)\n",
    "# [5613, 4086]\n",
    "\n",
    "# engine.ntd(prompt_ids=input_ids)\n",
    "# {'prompt_cnt': 257, 'result_by_token_id':\n",
    "# {13: {'cont_cnt': 1, 'prob': 0.0038910505836575876},\n",
    "# 297: {'cont_cnt': 1, 'prob': 0.0038910505836575876},\n",
    "# ...,\n",
    "# 30003: 'cont_cnt': 1, 'prob': 0.0038910505836575876}}, 'approx': False}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`result_by_token_id` is a `dict` that maps token id to the probability of that\n",
    "token as a continuation of the prompt.\n",
    "\n",
    "If the prompt cannot be found in the corpus, you will get an empty distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[306, 5360, 5613, 4086, 9068]\n",
      "{'prompt_cnt': 0, 'result_by_token_id': {}, 'approx': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "input_ids = tokenizer.encode('I love natural language processing')\n",
    "print(input_ids)\n",
    "# [306, 5360, 5613, 4086, 9068]\n",
    "\n",
    "print(engine.ntd(prompt_ids=input_ids[:-1]))\n",
    "# {'prompt_cnt': 0, 'result_by_token_id': {}, 'approx': False}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt_cnt': 1421, 'cont_cnt': 47, 'prob': 0.033075299085151305, 'suffix_len': 1}\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('hello world')\n",
    "print(engine.infgram_prob(prompt_ids=input_ids[:-1], cont_id=input_ids[-1]))\n",
    "#{'prompt_cnt': 257, 'cont_cnt': 76, 'prob': 0.29571984435797666, 'suffix_len': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = engine.infgram_ntd(prompt_ids=input_ids, max_support=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([13, 322, 1342, 1824, 29871, 29908])\n"
     ]
    }
   ],
   "source": [
    "tok_ids = result['result_by_token_id'].keys()\n",
    "print(tok_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<0x0A>', '▁and', '▁example', '▁program', '▁', '\"']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tok_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5613, 4086, 9068]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cnt': 76,\n",
       " 'approx': False,\n",
       " 'idxs': [69],\n",
       " 'documents': [{'doc_ix': 109291,\n",
       "   'doc_len': 521,\n",
       "   'disp_len': 10,\n",
       "   'metadata': '{\"path\": \"val.jsonl\", \"linenum\": 109291, \"metadata\": {\"meta\": {\"pile_set_name\": \"Wikipedia (en)\"}}}\\n',\n",
       "   'token_ids': [14881,\n",
       "    9608,\n",
       "    29879,\n",
       "    29889,\n",
       "    512,\n",
       "    5613,\n",
       "    4086,\n",
       "    9068,\n",
       "    29892,\n",
       "    670]}]}"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer.encode('natural language processing')\n",
    "print(input_ids)\n",
    "# [5613, 4086, 9068]\n",
    "\n",
    "engine.search_docs(input_ids=input_ids, maxnum=1, max_disp_len=10)\n",
    "\n",
    "# {'cnt': 76, 'approx': False, 'idxs': [54], 'documents': [{'doc_ix': 142405, 'doc_len': 19238, 'disp_len': 10, 'metadata': '', 'token_ids': [4475, 304, 9045, 2562, 322, 5613, 4086, 9068, 29889, 13]}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5613, 4086, 9068]], [[6483, 6509]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'cnt': 6,\n",
       " 'approx': False,\n",
       " 'idxs': [4],\n",
       " 'documents': [{'doc_ix': 68215,\n",
       "   'doc_len': 1527,\n",
       "   'disp_len': 20,\n",
       "   'metadata': '{\"path\": \"val.jsonl\", \"linenum\": 68215, \"metadata\": {\"meta\": {\"pile_set_name\": \"OpenWebText2\"}}}\\n',\n",
       "   'token_ids': [607,\n",
       "    7199,\n",
       "    4637,\n",
       "    2211,\n",
       "    1667,\n",
       "    7117,\n",
       "    29901,\n",
       "    4933,\n",
       "    6509,\n",
       "    29892,\n",
       "    5613,\n",
       "    4086,\n",
       "    9068,\n",
       "    29892,\n",
       "    322,\n",
       "    3061,\n",
       "    4564,\n",
       "    4110,\n",
       "    297,\n",
       "    427]}]}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# natural language processing AND deep learning\n",
    "cnf = [\n",
    "     [tokenizer.encode('natural language processing')],\n",
    "     [tokenizer.encode('deep learning')],\n",
    "]\n",
    "print(cnf)\n",
    "# [[[5613, 4086, 9068]], [[6483, 6509]]]\n",
    "\n",
    "engine.search_docs_cnf(cnf=cnf, maxnum=1, max_disp_len=20)\n",
    "# {'cnt': 6, 'approx': False, 'idxs': [2], 'documents': [{'doc_ix': 191568, 'doc_len': 3171, 'disp_len': 20, 'metadata': '', 'token_ids': [29889, 450, 1034, 13364, 508, 367, 4340, 1304, 304, 7945, 6483, 6509, 2729, 5613, 4086, 9068, 9595, 1316, 408, 10013]}]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, you can also use max_clause_freq and max_diff_tokens to control the behavior of CNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   improve the quality of developed ontology.<0x0A><0x0A>The remainder of the paper is structured as follows. In the following section \\[related-work\\] state of the art is presented with the review of existing methodologies for ontology development and approaches for ontology evaluation. After highlighting some drawbacks of current approaches section \\[ROD\\] presents the ROD approach. Short overview of the process and stages is given with the emphasis on ontology completeness indicator. The details of ontology evaluation and ontology completeness indicator are given in section \\[indicator\\], where all components (description, partition, redundancy and anomaly) that are evaluated are presented. In section \\[evaluation\\] evaluation and discussion about the proposed approach according to the results obtained in the experiment of **Financial Instruments and Trading Strategies (FITS)** is presented. Finally in section \\[conclusion-and-future-work\\] conclusions with future work are given.<0x0A><0x0A>Related work<0x0A>============<0x0A><0x0A>Review of related approaches<0x0A>----------------------------<0x0A><0x0A>Ontology is a vocabulary that is used for describing and presentation of a domain and also the meaning of that vocabulary. The definition of ontology can be highlighted from several aspects. From taxonomy [@corcho_methodologies_2003; @sanjuan_text_2006; @veale_analogy-oriented_2006] as knowledge with minimal hierarchical structure, vocabulary [@bechhofer_thesaurus_2001; @miller_wordnet:_1995] with words and synonyms, topic maps [@dong_hyo-xtm:_2004; @park_xml_2002] with the support of traversing through large amount of data, conceptual model [@jovanovic_achieving_2005; @mylopoulos_information_1998] that emphasizes more complex knowledge and logic theory [@corcho_methodologies_2003; @dzemyda_optimization_2009; @waterson_verifying_1999] with very complex and consistent knowledge.<0x0A><0x0A>Ontologies are used for various purposes such as natural language processing [@staab_system_1999], knowledge management [@davies_semantic_2006], information extraction [@wiederhold_mediators_1992], intelligent search engines [@heflin_searching_2000], digital libraries [@kesseler_schema_1996], business process modeling [@brambilla_software_2006; @ciuksys_reusing_2007; @magdalenic_dynamic_2009] etc. While the use of ontologies was primarily in the domain of academia, situation now improves with the advent of several methodologies for ontology manipulation. Existing methodologies for ontology development in general try to define the activities for ontology management, activities for ontology development and support activities. Several methodologies exist for ontology manipulation and will be briefly presented in the following section. CommonKADS [@schreiber_knowledge_1999] is in fact not a methodology for ontology development, but is focused towards knowledge management in information systems with analysis, design and implementation of knowledge. CommonKADS puts an emphasis to early stages of software development for knowledge management. Enterprise Ontology [@uschold_towards_1995] recommends three simple steps: definition of intention; capturing concepts, mutual relation and expressions based on concepts and relations; persisting ontology in one of the languages. This methodology is the groundwork for many other approaches and is also used in several ontology editors. METHONTOLOGY [@fernandez-lopez_building_1999] is a methodology for ontology creation from scratch or by reusing existing ontologies. The framework enables building ontology at conceptual level and this approach is very close to prototyping. Another approach is TOVE [@uschold_ontologies:_1996] where authors suggest using questionnaires that describe questions to which ontology should give answers. That can be very useful in environments where domain experts have very little expertise of knowledge modeling. Moreover authors of HCONE [@kotis_human_2003] present decentralized approach to ontology development by introducing regions\n"
     ]
    }
   ],
   "source": [
    "tok_ids = engine.get_doc_by_rank(s=0, rank=365362993, max_disp_len=1000)['token_ids']\n",
    "print(\"\".join(tokenizer.convert_ids_to_tokens(tok_ids)).replace('▁', ' '))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngram-projection-inductive-bias",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
